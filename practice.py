# -*- coding: utf-8 -*-
"""MLpractice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FADgu3ZRmboYGEqKkLSVbe3bv5wcDBlm

# PRACTICE




```
# This is formatted as code
```
"""

!pip install keras

!pip install -q matplotlib-venn

!apt-get -qq install -y libfluidsynth1

conda install scikit-learn

pip install -U scikit-learn

>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> digits = datasets.load_digits()

>>> print(digits.data)

>>> digits.target

>>> digits.images[0]

import matplotlib.pyplot as plt
import numpy as np

np.random.seed(19680801) #pseudo random number 
data = np.random.randn(2, 100)

fig, axs = plt.subplots(2, 2, figsize=(5, 5))
axs[0, 0].hist(data[0])
axs[1, 0].scatter(data[0], data[1])
axs[0, 1].plot(data[0], data[1])
axs[1, 1].hist2d(data[0], data[1])

plt.show()

fig=plt.figure()
fig.suptitle("No axis on this figure")
fig,ax_lst=plt.subplots(2,2)

from matplotlib import pyplot as plt

x=[1,2,3]
y=[4,2,1]


plt.title('info')
plt.xlabel(' X axix')
plt.ylabel('Y label')
plt.show()

from matplotlib import style
style.use("ggplot")

x=[1,2,3]
y=[3,2,6]

x2=[3,4,2]
y2=[8,6,9]

plt.plot(x,y,'c',label='Line 1',linewidth=10)
plt.plot(x2,y2,'r',label="Line 2",linewidth=5)

plt.title('Info')
plt.xlabel('x')
plt.ylabel('y')

plt.legend()

plt.grid(True,color='k')
plt.show()

import matplotlib.pyplot as plt
plt.bar([1,2,7],[2,3,6],label="Ex 1")
plt.bar([4,5,8],[6,7,1],label="Ex 2",color='g')
plt.legend()

population_ages=[13,5,3,6,3,4,3,5,3,5,2,6,8,3,2,4,52,4,5]
bins=[0,10,20,30,40,50,60,70,80,90,100,120,130,140,150]
plt.hist(population_ages,bins,histtype='bar',rwidth=1)
plt.legend()
plt.show()

x=[3,4,5,3,5]
y=[4,3,5,3,2]
plt.scatter(x,y,label='s',color='g')

a=[4,5,4,3,6]
b=[5,4,2,5,5]
c=[4,3,6,2,5]
d=[5,4,2,7,4]
e=[4,6,4,3,2]

plt.plot([],[],color='m',label='b',linewidth=5)
plt.plot([],[],color='c',label='c',linewidth=5)
plt.plot([],[],color='r',label='d',linewidth=5)
plt.plot([],[],color='k',label='e',linewidth=5)

plt.stackplot(a,b,c,d,e,colors=['m','c','r','k'])

slices=[7,2,2,13]
act=['a','b','c','d']
cols=['m','c','r','b']
plt.pie(slices,labels=act,colors=cols,startangle=90,shadow=True,explode=(0,0.1,0,0.1),autopct='%1.1f%%')

import numpy as np
a=np.array([(3,2,4),(4,3,5)])
print(a)

import numpy as npggplot
  import time
  import sys
  s=range(1000)
  print(sys.getsizeof(5)*len(s))
  D=np.arange(1000)
  print(D.size*D.itemsize)

import numpy as np
import time
size=1
l1=range(size)
l2=range(size)
a1=np.arange(size)
a2=np.arange(size)
start=time.time()
result=[(x,y) for x,y in zip(l1,l2)]
print((time.time()-start)*1000)
start=time.time()
result=a1+a2
print((time.time()-start)*1000)

import numpy as np
a=np.array([2,3,4])
print(a.shape)

!pip install -q matplotlib-venn

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.utils import shuffle
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

from google.colab import auth
auth.authenticate_user()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# % matplotlib inline
import math

#to import files
from google.colab import files

"""# ACME"""

#uploading files from pc
uploaded=files.upload()

#reading the uploaded file
pd.read_csv("acme.csv.csv")

acme=pd.read_csv("acme.csv.csv")
acme.head(10)

print(str(len(acme.index)))

sns.countplot(x='month',hue='acme',data=acme)

"""# TITANIC LOGISTIC REGRESSION"""

#to import a dataset online
!wget https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/Titanic.csv

titanic_data = pd.read_csv("Titanic.csv")
titanic_data.head(10)

print(str(len(titanic_data.index)))

sns.countplot(x='Survived',data=titanic_data)

sns.countplot(x="Survived",hue="Sex",data=titanic_data)

titanic_data['Freq'].plot.hist()

from google.colab import files
uploaded=files.upload()

pd.read_csv("Tit.csv")

titanic = pd.read_csv("Tit.csv")

sns.countplot(x='Survived',data=titanic)

sns.countplot(x="Survived",hue="Sex",data=titanic)

titanic['Age'].plot.hist()

sns.countplot(x="Survived",hue="Pclass",data=titanic)

sns.countplot(x="Survived",hue="Embarked",data=titanic)

titanic['Fare'].plot.hist()

titanic['Fare'].plot.hist(bins=20,figsize=(10,5))

titanic.info()

sns.countplot(x="SibSp",data=titanic)

titanic.isnull()

titanic.isnull().sum()

sns.heatmap(titanic.isnull(),cmap='viridis')

sns.boxplot(x='Pclass',y='Age',data=titanic)

titanic.head(5)

titanic.dropna(inplace=True)

titanic.head(5)

sns.heatmap(titanic.isnull())

sns.heatmap(titanic.isnull(),cmap='viridis')

titanic.isnull().sum()

#have to change string values using pandas entering dummy values
titanic.head(2)

pd.get_dummies(titanic['Sex'])

#Since ome of the above column would be enough to understand if the person is male or female, we drop one of them 
sex=pd.get_dummies(titanic['Sex'],drop_first=True)
sex.head(5)

embark=pd.get_dummies(titanic['Embarked'])
embark.head(5)

embark=pd.get_dummies(titanic['Embarked'],drop_first=True)
embark.head(5)

p=pd.get_dummies(titanic['Pclass'])
p.head(5)

p=pd.get_dummies(titanic['Pclass'],drop_first=True)
p.head(5)

titanic=pd.concat([titanic,sex,embark,p],axis=1)

titanic.head(5)

titanic.drop(['Sex','Embarked','PassengerId','Name','Ticket','Pclass','Cabin'],axis=1)

titanic.head(2)

X=titanic.drop('Survived',axis=1) 
#except teh survived column, rest are all independent variables
y=titanic['Survived'] 
#the dependent variable or the output we want to achieve

from sklearn.model_selection import train_test_split #it actually says from sklearn.cross_validation so check

X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.3,random_state=1)

from sklearn.linear_model import LogisticRegression

logmodel=LogisticRegression()

logmodel.fit(X_train,y_train)

predictions= logmodel.predict(X_test)

from sklearn.metrics import classification_report

classification_report(y_test,predictions)

from sklearn.metrics import confusion_matrix

confusion_matrix(y_test,predictions)

from sklearn.metrics import accuracy_score

accuracy_score(y_test,predictions)

X

y

"""# **IRIS**"""

ffrom sklearn.datasets import load_iris

iris=load_iris()
type(iris)

print iris.data

print iris.feature_names

print iris.target

print iris.target_names

print type(iris.data)

print type(iris.target)

print iris.data.shape

print iris.target.shape

X=iris.data
y=iris.target

print iris.DESCR

from sklearn.utils import shuffle

x,Y= shuffle(X,y,random_state=0)

print(x)

from sklearn.model_selection import train_test_split

x_train,x_test,Y_train,Y_test = train_test_split(x,Y,test_size=0.3,random_state=42)

Y_train.shape

from sklearn.linear_model import LogisticRegression

logmodel=LogisticRegression()

logmodel.fit(x_train,Y_train)

Y_predictions=logmodel.predict(x_test)

from sklearn.metrics import accuracy_score
acc=accuracy_score(Y_test,Y_predictions)

print(acc)

from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics

digits=load_digits()

digits.data.shape

digits.target.shape

with open('some_file','w') as opened_file:
  opened_file.write('Hola!')

print(opened_file.mode)

print(opened_file.name)

X=digits.data
y=digits.target
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)

from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,y_train)

predictions=logmodel.predict(X_test)

from sklearn.metrics import accuracy_score
acc=accuracy_score(y_test,predictions)
print(acc)

from google.colab import files
uploaded=files.upload()

import pandas as pd
pd.read_csv('victims.csv')

import seaborn as sns

victims=pd.read_csv('victims.csv')
victims.head(3)

sns.countplot(x='Subgroup',hue='Year',data=victims)

victims.drop(['Area_Name','Subgroup'],axis=1)

from sklearn.model_selection import train_test_split
X_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)

from sklearn.linear_model import LogisticRegression

logmodel=LogisticRegression()

logmodel.fit(X_train,y_train)

victims.isnull()

victims.info()

victims.isnull().sum()

"""# READING FROM DOCX FILE"""

import re
from docx import Document
 
def docx_replace_regex(doc_obj, regex , replace):
 
    for p in doc_obj.paragraphs:
        if regex.search(p.text):
            inline = p.runs
            # Loop added to work with runs (strings with same style)
            for i in range(len(inline)):
                if regex.search(inline[i].text):
                    text = regex.sub(replace, inline[i].text)
                    inline[i].text = text
 
    for table in doc_obj.tables:
        for row in table.rows:
            for cell in row.cells:
                docx_replace_regex(cell, regex , replace)
 
 
 
regex1 = re.compile(r"your regex")
replace1 = r"your replace string"
filename = "jkt.docx"
doc = Document(filename)
docx_replace_regex(doc, regex1 , replace1)
doc.save('result1.docx')

pip install --pre python-docx

#to open a new document
from docx import Document
document = Document()

from docx import *
def ReadingTextDocuments(jkt):
  doc=docx.Document(jkt)
   
  completedText=[]
  for paragraph in docx.paragraphs:
    completedText.append(paragraph.text)
  return '\n' .join(completedText)
ReadingTextDocuments('jkt.docx')

#to open a exisitng document
document=Document('jkt.docx')
#to change the file name
document.save('new_file_name.docx')

#to open a file-like document
f = open('jkt.docx','rb') #read binary - rb
document= Document(f)
f.close()
document.save('document_name.docx')

document.paragraphs
document.paragraphs[2].text #gives the text

for i in range(0,50,1):
  if 'igsa' in document.paragraphs[i].text:
    print document.paragraph

'Some-word' in document.paragraphs[i].text
(document.paragraphs[i].text==('igsa')):

for par in document.paragraphs:  # to extract the whole text
  if 'igsa' in par.text:
     print(par.text)

from docx import *

# Open the .docx file
document = opendocx('jkt.docx')

# Search returns true if found    
search(document,'your search string')

from docx import Document
doc= Document('jkt.docx')

doc.paragraphs

for par in doc.paragraphs:
  print(par.text)

!pip install --upgrade -q gspread

from google.colab import auth
auth.authenticate_user()

import gspread
from oauth2client.client import GoogleCredentials

gc = gspread.authorize(GoogleCredentials.get_application_default())

sh = gc.create('A new spreadsheet')

# Open our new sheet and add some data.
worksheet = gc.open('A new spreadsheet').sheet1

cell_list = worksheet.range('A1:C2')

import random
for cell in cell_list:
  cell.value = random.randint(1, 5)

worksheet.update_cells(cell_list)
# Go to https://sheets.google.com to see your new spreadsheet.

for par in doc.paragraphs:
  for par in doc.paragraphs:
    if (par.text)=='igsa':
      print(par.text)

from docx import Document
from docx.enum.text import WD_COLOR_INDEX


document = Document('jkt.docx')


for para in document.paragraphs:
    for run in para.runs:
        run.font.highlight_color = WD_COLOR_INDEX.WHITE
document.save('jkt.docx')

# load an example dataset
from vega_datasets import data
cars = data.cars()

# plot the dataset, referencing dataframe column names
import altair as alt
alt.Chart(cars).mark_bar().encode(
  x=alt.X('Miles_per_Gallon', bin=True),
  y='count()',
  color='Origin'
)

predictionyear=2000
self.name=name



"""# DOCX 2"""

pip install --pre python-docx

#to open a new document
from docx import Document

#to open a exisitng document
document=Document('jkt.docx')

#to open a file-like document
f = open('jkt.docx','rb') #read binary - rb
document= Document(f)
f.close()

document.paragraphs
document.paragraphs[2].text #gives the text

document.paragraphs

for par in document.paragraphs:
  print(par.text)

for par in document.paragraphs:  # to extract the whole text
  if (('igsa') in par.text):
     print(par.text)

a=input("Enter a word you want to search")
for i in range(0,50,1):
  if (document.paragraphs[i].text)=='igsa':
    document.paragraphs[i].text.replace('\033[1m a \033[0m')

"""# SVM"""

import numpy
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.datasets.samples_generator import make_blobs

X,y=make_blobs(n_samples=40,centers=2,random_state=20)
clf=svm.SVC(kernel='linear',C=1000)
clf.fit(X,y)
plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired)

from sklearn.datasets.samples_generator import make_blobs
from sklearn import svm
import numpy as np
import matplotlib.pyplot as plt
X,y=make_blobs(n_samples=40,centers=2,random_state=20)
clf=svm.SVC(kernel='linear',C=1)
clf.fit(X,y)
plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired)

newData=[[3,4],[5,6]]
print(clf.predict(newData)) #0 implies blue side and 1 implies red side

clf=svm.SVC(kernel='linear',C=1)
clf.fit(X,y)
plt.scatter(X[:,0],X[:,1],c=y,s=30,cmap=plt.cm.Paired)
ax=plt.gca()
xlim=ax.get_xlim()
ylim=ax.get_ylim()
xx=np.linspace(xlim[0],xlim[1],30)
yy=np.linspace(ylim[0],ylim[1],30)
YY,XX=np.meshgrid(yy,xx)
xy=np.vstack([XX.ravel(),YY.ravel()]).T
Z=clf.decision_function(xy).reshape(XX.shape)

ax.contour(XX,YY,Z,colors='k',levels=[-1,0,1],alpha=0.5,linestyles=['--','-','--'])

ax.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=100,linewidth=1,facecolors='none')
plt.show()

from sklearn import datasets
iris=datasets.load_iris()



type(iris)
print iris.data



y=iris.target
X=iris.feature_names

print iris.feature_names

print iris.target

model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=150,150,3),tf.keras.layers.Dense(512,activation='relu'),tf.keras.layerrs.Dense(3,activation='softmax')])
model.compile(loss='categorical_crossentropy',optimizer='rmsprop')

model.fit(...,epochs=100)



"""# **MNIST**"""

import tensorflow as tf
from tensorflow import keras

import numpy as np
import matplotlib.pyplot as plt

print(tf.__version__)

fashion_mnist=keras.datasets.fashion_mnist

tf.keras.datasets.fashion_mnist.load_data()

(train_images,train_labels),(test_images,test_labels)=fashion_mnist.load_data()

class_names=['T/shirt','Trouser','pullover','dress','coat','sandal','shirt','sneaker','bag','boot']

train_images.shape

len(train_labels)

train_images

plt.figure()
plt.imshow(train_images[0])
plt.colorbar()
plt.grid(False)
plt.show()

plt.figure()
plt.imshow(train_images[1])
plt.colorbar()
plt.grid(False)
plt.show()

plt.figure()
plt.imshow(train_images[2])
plt.colorbar()
plt.grid(False)
plt.show()

plt.figure()
plt.imshow(train_images[59])
plt.colorbar()
plt.grid(True)
plt.show()

train_images=train_images/255.0
test_images=test_images/255.0

plt.figure(figsize=(10,10))
for i in range(25):
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(train_images[i],cmap=plt.cm.binary)
  plt.xlabel(class_names[train_labels[i]])
plt.show()

plt.figure(figsize=(50,50))
for i in range(50):
  plt.subplot(10,10,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(train_images[i],cmap=plt.cm.binary)
  plt.xlabel(class_names[train_labels[i]])
plt.show()

model=keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(128,activation=tf.nn.relu),
    keras.layers.Dense(10,activation=tf.nn.softmax)
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy', #we use categorical_crossentropy if inputs are one hot encoded, else wwe add sparse
              metrics=['accuracy'])

model.fit(train_images,train_labels,epochs=10)

test_loss,test_acc=model.evaluate(test_images,test_labels)
  print('Test accuracy:',test_acc) #88 percent ouput shows overfitting:model performs better on training than testing data

predictions=model.predict(test_images)

predictions[10]

np.argmax(predictions[0])

test_labels[0]

"""# **CIFA**R"""

tf.keras.datasets.cifar10.load_data()

import pickle
import numpy as np
import

import matplotlib.pyplot as plt
import numpy as np
from scipy import signal
from sklearn.decompostion import FastICA, PCA

pip install Django

import torch

x=torch.empty(5,3)
print(x)

x=torch.rand(5,3)
print(x)

x=torch.zeros(5,3,dtype=torch.long)
print(x)

x=torch.tensor([5.5,3])
print(x)

x=x.new_ones(5,3,dtype=torch.double)
print(x)

x=torch.randn_like(x,dtype=torch.float)
print(x)

"""# BSS

# BSS IMAGE
"""

!add-apt-repository ppa:shogun-toolbox/stable
!apt-get update
!apt-get install libshogun18

import numpy as np
from scipy.io import wavfile
from scipy.signal import resample

!apt-get install python-shogun

from shogun.Features import RealFeatures

from PIL import Image
import numpy as np
s1 = np.asarray(Image.open('monalisa.jpg').convert('L'))
s2 = np.asarray(Image.open('starrynight.jpg').convert('L'))

rows = s1.shape[0]
cols = s1.shape[1]

import pylab as pl

f, (ax1,ax2) = pl.subplots(1,2)
ax1.imshow(s1,cmap=pl.gray())
ax2.imshow(s2)
pl.show()

pip install python-resize-image

from PIL import Image

from resizeimage import resizeimage


with open('monalisa.jpg', 'r+b') as f:
    with Image.open(f) as image:
        cover = resizeimage.resize_cover(image, [200, 100])
        cover.save('test-image-cover.jpeg', image.format)

with open('starrynight.jpg', 'r+b') as f:
    with Image.open(f) as image:
        cover = resizeimage.resize_cover(image, [200, 100])
        cover.save('test-image-cover.jpeg', image.format)

f, (ax1,ax2) = pl.subplots(1,2)
ax1.imshow(s1,cmap=pl.gray())
ax2.imshow(s2)
pl.show()

from numpy import c_

S = np.c_[s1.flatten(),s2.flatten()].T
A = np.array([[1,0.5],[0.5,1]])

X= np.dot(A,S)

"""# ****"""

import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib

dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'
data = pd.read_csv(dataset_url,sep=';')

print data.head() # first 5 readings

print(data.shape)

print (data)

print (data.describe())

y=data.quality
x=data.drop('quality',axis=1)

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=123,stratify=y,)

x_train

x_train_scaled = preprocessing.scale(x_train)
print(x_train_scaled)

scaler=preprocessing.StandardScaler().fit(x_train)

x_test_scaled = scaler.transform(x_test)

pipeline = make_pipeline(preprocessing.StandardScaler(),RandomForestRegressor(n_estimators=100))

print pipeline.get_params()

hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],
                  'randomforestregressor__max_depth': [None, 5, 3, 1]}

clf=GridSearchCV(pipeline,hyperparameters,cv=10)
clf.fit(x_train,y_train)

print clf.

training_data=[['Green',3,'Mango',['Yellow',3,'Mango'],['Red',1,'Grape'],['Red',1,'Grape'],['Yellow',3,'Lemon']]]
header = ['color','diameter','label']


def unique_values(rows,col):
  return set([rows[col] for row in rows])
#uniquevalues(training_data,0) - training_data is passed as row and 0th column is colour 
#uniquevalues(training_data,1) - training_data is passed as row and 1st column is diameter 


def class_counts(rows):
  counts={}
  for row in rows:
    label=row[-1]
    if label not in counts:
      counts[label]=0
    counts[label]=counts[label+1]
  return counts  
#class_counts(training_data)


def is_numeric(value):
  return isinstance(value,int) or isinstance(value,float)
#The isinstance() function returns True if the specified object is of the specified type, otherwise False. ex: x = isinstance(5, int)
#is_numeric(7) - true
#is_numeric('red') - false


class Question:
  
  def __init__(self,column,value):
    self.column=column
    self.vaue=value
    
  def match(self,example):
    val = example[self.column]
    if is_numeric(val):
      return val>=self.value
    else:
      return val == self.value
  
  def __repr__(self):
    #helper function to print question in readable format
    condition = '=='
    if is_numeric(self.value):
      condition='>='
    return 'Is %s %s %s?' % (header[self.colum],condition,str(self.value)) 
  
  def partition(rows,questions):
    true_rows, false_rows =[], [] 
    for row in rows:
      if question.match(row):
        true_rows.append(row)
      else:
        false_rows.append(row)
    return true_rows, false_rows
  
  def gini(rows):
    #to calculate gini impurity for the rows
    counts=class_counts(rows)
    impurity = 1
    for lbl in counts:
      prob_of_lbl = counts[lbl]/ float(len(rows))
      impurity=prob_of_lbl**2 - impurity
      return impurity
  
  def info_gain(left,right,current_uncertainty):
    p = float(len(left)) / (len(left) + len(right))
    return current_uncertainty - p * gini(left) - (1-p) * gini (right)
  
  def find_best_split(rows):
    best_gain = 0
    best_question = None
    current_uncertainty = gini(rows)
    n_features = len(rows[0])-1
    
    for col in range(n_features):
      values=set([row[col] for ])

"""# NAIVE BAYES"""

import sklearn 
from sklearn import datasets
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB

dataset = datasets.load_iris()

model = GaussianNB()
model.fit(dataset.data,dataset.target)

print(model)

"""# New Section"""

expected=dataset.target
predicted=model.predict(dataset.data)

print(metrics.classification_report(expected,predicted))
print(metrics.confusion_matrix(expected,predicted))

import sys
import numpy
import pandas
import matplotlib
import seaborn 
import scipy
import sklearn

print('Python: {}'.format(sys.version))
print('Numpy:{}'.format(numpy.__version__))
print('matplotlib:{}'.format(matplotlib.__version__))
print('pandas:{}'.format(pandas.__version__))
print('scipy:{}'.format(scipy.__version__))
print('seaborn:{}'.format(seaborn.__version__))
print('sklearn:{}'.format(sklearn.__version__))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv("creditcard.csv")

print(data.columns)

print(data.shape)

print(data.describe)

print(data.describe())

#if we we want to use only some amount of the data , we use the below code
# data=data.sample(frac=0.1,random_state=1)
# this will give us only 0.1 or 10% of the total data

data.hist(figsize=(20,20))
plt.show()

Fraud=data[data['Class']==1]
Valid=data[data['Class']==0]

outlier_fraction=len(Fraud)/float(len(Valid))
print(outlier_fraction)
print('Fraud cases:{}'.format(len(Fraud)))
print('Valid cases:{}'.format(len(Valid)))

corrmat=data.corr()

fig = plt.figure(figsize=(12,9))

sns.heatmap(corrmat,vmax=.8)
plt.show()

columns=data.columns.tolist()

columns=[c for c in columns if c not in ['Class']]

target='Class'

X=data[columns]
y=data[target]
print(X.shape)
print(y.shape)

from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor

state = 1
classifiers = {
    'Isolation Forest':IsolationForest(max_samples=len(X),
                                      contamination=outlier_fraction,
                                      random_state=state),
    'Local Outlier Factor': LocalOutlierFactor(n_neighbors=20,
                                               contamination=outlier_fraction)
}

n_outliers=len(Fraud)
for i, (clf_name,clf) in enumerate(classifiers.items()):
  if clf_name=='Local Outlier Factor':
    y_pred=clf.fit_predict(X)
    scores_pred=clf.negative_outlier_factor_
  else:
    clf.fit(X)
    scores_pred=clf.decision_function(X)
    y_pred=clf.predict(X)
    
    y_pred[y_pred==1]=0
    y_pred[y_pred==-1]=1
    n_errors(y_pred!=y).sum()
    print('{}:{}'.format(clf_name,n_errors))
    print(accuracy_score(y,y_pred))
    print(classification_report(y,y_pred))

pip install mlagents

pip install mlagents

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage

# %matplotlib inline

from google.colab import files
uploaded=files.upload()

# Loading the data (cat/non-cat)
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()

